{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76240346-b656-4c4d-b29e-965cf42b269d",
   "metadata": {},
   "source": [
    "# 吴恩达深度学习课程作业L2W7\n",
    "\n",
    "* TensorFlow入门\n",
    "* 使用TensorFlow构建dnn网络\n",
    "\n",
    "## HW参考\n",
    "\n",
    "1. [视频链接](https://mooc.study.163.com/university/deeplearning_ai#/c)\n",
    "2. [作业链接](https://github.com/suqi/deeplearning_andrewng/tree/master/Course2-DL-tuning/week7)\n",
    "3. [tensorflow2指南](https://www.tensorflow.org/guide)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434f3872-b45a-49a7-aed3-e42254c60b7a",
   "metadata": {},
   "source": [
    "## TensorFlow基础知识\n",
    "\n",
    "### Eager Execution=即刻执行\n",
    "\n",
    "TensorFlow 的 Eager Execution 是一种命令式编程环境，可立即评估运算，无需构建计算图：运算会返回具体的值，而非构建供稍后运行的计算图\n",
    "\n",
    "启用 Eager Execution 会改变 TensorFlow 运算的行为方式 - 现在它们会立即评估并将值返回给 Python。tf.Tensor 对象会引用具体值，而非指向计算图中节点的符号句柄。由于无需构建计算图并稍后在会话中运行，可以轻松使用 print() 或调试程序检查结果。评估、输出和检查张量值不会中断计算梯度的流程\n",
    "\n",
    "#### 设置和用法\n",
    "\n",
    "在 Tensorflow 2.0 中，默认启用 Eager Execution。以运行 TensorFlow 运算，结果将立即返回"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "362429e4-8c19-4e3d-90d9-7035fb47d5db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-09 19:51:50.036372: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import cProfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "601eacdc-4b0f-40b9-8f09-7a782cee680b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.executing_eagerly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10f669f1-0b48-4e82-8f92-5204e7620a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-09 19:54:29.773832: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-10-09 19:54:29.776461: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2021-10-09 19:54:29.949713: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:06:00.0 name: Tesla P40 computeCapability: 6.1\n",
      "coreClock: 1.531GHz coreCount: 30 deviceMemorySize: 22.38GiB deviceMemoryBandwidth: 323.21GiB/s\n",
      "2021-10-09 19:54:29.949767: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-10-09 19:54:29.954709: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
      "2021-10-09 19:54:29.954795: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n",
      "2021-10-09 19:54:29.956852: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2021-10-09 19:54:29.957292: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2021-10-09 19:54:29.962309: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2021-10-09 19:54:29.963437: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\n",
      "2021-10-09 19:54:29.963690: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
      "2021-10-09 19:54:29.968263: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2021-10-09 19:54:29.975665: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-10-09 19:54:29.977810: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:06:00.0 name: Tesla P40 computeCapability: 6.1\n",
      "coreClock: 1.531GHz coreCount: 30 deviceMemorySize: 22.38GiB deviceMemoryBandwidth: 323.21GiB/s\n",
      "2021-10-09 19:54:29.977848: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-10-09 19:54:29.977881: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
      "2021-10-09 19:54:29.977902: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n",
      "2021-10-09 19:54:29.977927: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2021-10-09 19:54:29.977947: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2021-10-09 19:54:29.977967: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2021-10-09 19:54:29.977986: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\n",
      "2021-10-09 19:54:29.978006: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
      "2021-10-09 19:54:29.981856: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2021-10-09 19:54:29.981920: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-10-09 19:54:30.616622: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2021-10-09 19:54:30.616667: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \n",
      "2021-10-09 19:54:30.616676: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \n",
      "2021-10-09 19:54:30.620187: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 21298 MB memory) -> physical GPU (device: 0, name: Tesla P40, pci bus id: 0000:06:00.0, compute capability: 6.1)\n",
      "2021-10-09 19:54:30.626572: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
      "2021-10-09 19:54:30.883575: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello, [[4.]]\n"
     ]
    }
   ],
   "source": [
    "x = [[2.]]\n",
    "m = tf.matmul(x, x)\n",
    "print('hello, {}'.format(m))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e186d94-51a0-4a5a-baf9-a54adfc90108",
   "metadata": {},
   "source": [
    "Eager Execution 可以很好地配合 NumPy 使用。NumPy 运算接受 tf.Tensor 参数。TensorFlow tf.math 运算会将 Python 对象和 NumPy 数组转换为 tf.Tensor 对象。tf.Tensor.numpy 方法会以 NumPy ndarray 的形式返回该对象的值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bfd0eb83-471e-4943-9307-a8fd159f852a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1 2]\n",
      " [3 4]], shape=(2, 2), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant([[1,2], [3,4]])\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6606ccde-50f5-4fd9-bc95-48cc2a8605dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[2 3]\n",
      " [4 5]], shape=(2, 2), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "b = tf.add(a, 1)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1c853cb-7850-4d68-9330-7b2cddaa674b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 2  6]\n",
      " [12 20]], shape=(2, 2), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "print(a*b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "edf58aef-581e-4f0f-b65c-fc7ce9fe98bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2  6]\n",
      " [12 20]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "c = np.multiply(a, b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0412c1f1-2fca-4fe2-bb18-ff6d7b279afc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [3 4]]\n"
     ]
    }
   ],
   "source": [
    "print(a.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e715fef-671d-4e6a-b58d-ecf20c94cbc0",
   "metadata": {},
   "source": [
    "#### 动态控制流\n",
    "\n",
    "Eager Execution 的一个主要优势是，在执行模型时，主机语言的所有功能均可用。因此，编写 fizzbuzz 之类的代码会很容易"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28f53884-c100-4442-b93c-eac95994faa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fizzbuzz(max_num):\n",
    "    counter = tf.constant(0)\n",
    "    max_num = tf.convert_to_tensor(max_num)\n",
    "    for num in range(1, max_num.numpy()+1):\n",
    "        num = tf.constant(num)\n",
    "        if int(num % 3) == 0 and int(num % 5) == 0:\n",
    "            print('FizzBuzz')\n",
    "        elif int(num % 3) == 0:\n",
    "            print('Fizz')\n",
    "        elif int(num % 5) == 0:\n",
    "            print('Buzz')\n",
    "        else:\n",
    "            print(num.numpy())\n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2df2fcf-5b22-4eb9-8d9d-1c9d6e7268a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "Fizz\n",
      "4\n",
      "Buzz\n",
      "Fizz\n",
      "7\n",
      "8\n",
      "Fizz\n",
      "Buzz\n",
      "11\n",
      "Fizz\n",
      "13\n",
      "14\n",
      "FizzBuzz\n"
     ]
    }
   ],
   "source": [
    "fizzbuzz(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b22561-2417-4df2-9f09-538b13deb395",
   "metadata": {},
   "source": [
    "#### Eager训练\n",
    "\n",
    "##### 计算梯度\n",
    "\n",
    "自动微分对实现机器学习算法（例如用于训练神经网络的反向传播）十分有用。在 Eager Execution 期间，请使用 tf.GradientTape 跟踪运算以便稍后计算梯度。\n",
    "\n",
    "您可以在 Eager Execution 中使用 tf.GradientTape 来训练和/或计算梯度。这对复杂的训练循环特别有用。\n",
    "\n",
    "由于在每次调用期间都可能进行不同运算，所有前向传递的运算都会记录到“条带”中。要计算梯度，请反向播放条带，然后丢弃。特定 tf.GradientTape 只能计算一个梯度；后续调用会引发运行时错误"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c27537e5-0853-4a7c-ada5-26e9dce40f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[2.]], shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "w = tf.Variable([[1.0]])\n",
    "with tf.GradientTape() as tape:  # 跟踪运算以便稍后计算梯度\n",
    "    loss = w * w\n",
    "grad = tape.gradient(loss, w)\n",
    "print(grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb1d861-6f6d-4850-8f0c-20c940dfbf7c",
   "metadata": {},
   "source": [
    "##### 训练模型\n",
    "\n",
    "以下示例创建了一个多层模型，该模型会对标准 MNIST 手写数字进行分类。示例演示了在 Eager Execution 环境中构建可训练计算图的优化器和层 API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b0a99c24-daf8-4e00-b44f-919e79a7f178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch and format the mnist data\n",
    "(mnist_images, mnist_labels), _ = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((tf.cast(mnist_images[...,tf.newaxis]/255, tf.float32), tf.cast(mnist_labels,tf.int64)))  # 归一化并创建数据流\n",
    "dataset = dataset.shuffle(1000).batch(32)  # shuffle and batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6a29cc6c-60d9-4c42-b78a-8a36fd6f4a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits:  [[-0.05472895  0.16269219  0.09026919  0.11115909 -0.00300851  0.11862857\n",
      "   0.05138586 -0.08186927  0.00709331 -0.0081394 ]]\n"
     ]
    }
   ],
   "source": [
    "# build the model\n",
    "mnist_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(16, [3,3], activation='relu', input_shape=(None, None, 1)),\n",
    "    tf.keras.layers.Conv2D(16, [3,3], activation='relu'),\n",
    "    tf.keras.layers.GlobalAveragePooling2D(),\n",
    "    tf.keras.layers.Dense(10)\n",
    "])\n",
    "\n",
    "# 即使没有训练，也可以在 Eager Execution 中调用模型并检查输出\n",
    "for images, labels in dataset.take(1):\n",
    "    print('logits: ', mnist_model(images[0:1]).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e126356-0714-4b99-af0f-1ce0e15406ba",
   "metadata": {},
   "source": [
    "虽然 Keras 模型有内置训练循环（使用 fit 方法），但有时您需要进行更多自定义。下面是一个使用 Eager Execution 实现训练循环的示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "645625ad-34df-49e9-b1d1-2c2d0ea877dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 finished.\n",
      "epoch 1 finished.\n",
      "epoch 2 finished.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'loss [entropy]')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "loss_history = []\n",
    "\n",
    "# 注：请在 tf.debugging 中使用断言函数检查条件是否成立。这在 Eager Execution 和计算图执行中均有效\n",
    "def train_step(images, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = mnist_model(images, training=True)\n",
    "        tf.debugging.assert_equal(logits.shape, (32, 10))\n",
    "        loss_value = loss_object(labels, logits)\n",
    "    \n",
    "    loss_history.append(loss_value.numpy().mean())\n",
    "    grads = tape.gradient(loss_value, mnist_model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, mnist_model.trainable_variables))\n",
    "\n",
    "\n",
    "def train(epochs):\n",
    "    for epoch in range(epochs):\n",
    "        for (batch, (images, labels)) in enumerate(dataset):\n",
    "            train_step(images, labels)\n",
    "        print('epoch {} finished.'.format(epoch))\n",
    "\n",
    "\n",
    "train(3)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(loss_history)\n",
    "plt.xlabel('batch #')\n",
    "plt.ylabel('loss [entropy]')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f4b9d2-db36-47ca-afe9-a8b18543356f",
   "metadata": {},
   "source": [
    "##### 变量和优化器\n",
    "\n",
    "tf.Variable 对象会存储在训练期间访问的可变、类似于 tf.Tensor 的值，以更简单地实现自动微分。\n",
    "\n",
    "变量的集合及其运算方法可以封装到层或模型中。有关详细信息，请参阅自定义 Keras 层和模型。层和模型之间的主要区别在于模型添加了如下方法：Model.fit、Model.evaluate 和 Model.save。\n",
    "\n",
    "例如，上面的自动微分示例可以改写为"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3dc92941-223d-4ed8-89d7-72294b1707dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000,)\n",
      "initial loss: 68.43151092529297\n",
      "loss at epoch 000: 65.803\n",
      "loss at epoch 020: 30.265\n",
      "loss at epoch 040: 14.222\n",
      "loss at epoch 060: 6.976\n",
      "loss at epoch 080: 3.702\n",
      "loss at epoch 100: 2.221\n",
      "loss at epoch 120: 1.551\n",
      "loss at epoch 140: 1.248\n",
      "loss at epoch 160: 1.111\n",
      "loss at epoch 180: 1.049\n",
      "loss at epoch 200: 1.020\n",
      "loss at epoch 220: 1.008\n",
      "loss at epoch 240: 1.002\n",
      "loss at epoch 260: 0.999\n",
      "loss at epoch 280: 0.998\n",
      "W = 3.005833864212036, B = 1.9850844144821167\n"
     ]
    }
   ],
   "source": [
    "class Linear(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Linear, self).__init__()\n",
    "        self.W = tf.Variable(5., name='weight')\n",
    "        self.B = tf.Variable(10., name='bias')\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        return self.W * inputs + self.B\n",
    "\n",
    "# a toy dataset of points around 3*x + 2\n",
    "num_examples = 2000\n",
    "training_inputs = tf.random.normal([num_examples])\n",
    "print(training_inputs.shape)\n",
    "noise = tf.random.normal([num_examples])\n",
    "training_outputs = training_inputs * 3 + 2 + noise\n",
    "\n",
    "# the loss function to be optimized\n",
    "def loss(model, inputs, targets):\n",
    "    error = model(inputs) - targets\n",
    "    return tf.reduce_mean(tf.square(error))\n",
    "\n",
    "# grad\n",
    "def grad(model, inputs, target):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss_value = loss(model, inputs, target)\n",
    "    return tape.gradient(loss_value, [model.W, model.B])\n",
    "\n",
    "# 下一步，\n",
    "# 创建模型，\n",
    "# 损失函数对模型参数的导数，\n",
    "# 基于导数的参数更新\n",
    "model = Linear()\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "print('initial loss: {}'.format(loss(model, training_inputs, training_outputs)))\n",
    "\n",
    "epochs = 300\n",
    "for epoch in range(epochs):\n",
    "    grads = grad(model, training_inputs, training_outputs)\n",
    "    optimizer.apply_gradients(zip(grads, [model.W, model.B]))\n",
    "    if epoch % 20 == 0:\n",
    "        print('loss at epoch {:03d}: {:.3f}'.format(epoch, loss(model, training_inputs, training_outputs)))\n",
    "\n",
    "# print w and b\n",
    "print('W = {}, B = {}'.format(model.W.numpy(), model.B.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94637cf-0a68-4c29-9b94-d6b5d3293813",
   "metadata": {},
   "source": [
    "<font size=5 color=red>注：变量将一直存在，直至删除对 Python 对象的最后一个引用，并删除该变量</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4d16f3-e672-4682-a64f-a08f8bc594b1",
   "metadata": {},
   "source": [
    "##### 基于对象的保存\n",
    "\n",
    "tf.keras.Model 包括一个方便的 save_weights 方法，您可以通过该方法轻松创建检查点："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3a617b06-d79e-486b-95d5-65bd65365a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=10.0>\n",
      "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=2.0>\n",
      "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=11.0>\n",
      "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=2.0>\n"
     ]
    }
   ],
   "source": [
    "save_path = './temp/weights'\n",
    "model.save_weights(save_path)\n",
    "status = model.load_weights(save_path)\n",
    "# status\n",
    "\n",
    "# 您可以使用 tf.train.Checkpoint 完全控制此过程\n",
    "x = tf.Variable(10.)\n",
    "print(x)\n",
    "checkpoint = tf.train.Checkpoint(x=x)\n",
    "x.assign(2.)\n",
    "print(x)\n",
    "checkpoint_path = './temp/ckpt/'\n",
    "checkpoint.save(checkpoint_path)\n",
    "x.assign(11.)  # change the variable after saving\n",
    "print(x)\n",
    "# restore values from the checkpoint\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_path))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73eb9eee-335e-47f8-8c29-7fefa93f7ad2",
   "metadata": {},
   "source": [
    "要保存和加载模型，tf.train.Checkpoint 会存储对象的内部状态，而无需隐藏变量。要记录 model、optimizer 和全局步骤的状态，请将它们传递到 tf.train.Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2d15efa9-ce73-443c-8832-ae91f541463b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f3415cb0100>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(16, [3,3], activation='relu'),\n",
    "    tf.keras.layers.GlobalAveragePooling2D(),\n",
    "    tf.keras.layers.Dense(10)\n",
    "])\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "checkpoint_dir = './temp/model_dir'\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "checkpoint_path = os.path.join(checkpoint_dir, 'ckpt')\n",
    "root = tf.train.Checkpoint(optimizer=optimizer, model=model)\n",
    "root.save(checkpoint_path)\n",
    "root.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bee9a7b-05cd-4d73-ae8f-91d373c10934",
   "metadata": {},
   "source": [
    "注：在许多训练循环中，会在调用 tf.train.Checkpoint.restore 后创建变量。这些变量将在创建后立即恢复，并且可以使用断言来确保检查点已完全加载。有关详细信息，请参阅检查点训练指南"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b44a42-b245-4abd-bd3d-2045dd5012b9",
   "metadata": {},
   "source": [
    "##### 面向对象的指标\n",
    "\n",
    "tf.keras.metrics 会被存储为对象。可以通过将新数据传递给可调用对象来更新指标，并使用 tf.keras.metrics.result 方法检索结果，例如"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "95d58653-b64c-4387-bf70-d8d8276d3aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(2.5, shape=(), dtype=float32)\n",
      "tf.Tensor(5.5, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "m = tf.keras.metrics.Mean('loss')  # Computes the (weighted) mean of the given values\n",
    "m(0)\n",
    "m(5)\n",
    "print(m.result())\n",
    "m([8, 9])\n",
    "print(m.result())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061f0faf-ce5c-4c7c-a1c9-f32aede9ee5d",
   "metadata": {},
   "source": [
    "##### 摘要和TensorBoard\n",
    "\n",
    "[TensorBoard](https://tensorflow.google.cn/tensorboard)是一种可视化工具，用于了解、调试和优化模型训练过程。它使用在执行程序时编写的摘要事件。\n",
    "\n",
    "您可以在 Eager Execution 中使用 tf.summary 记录变量摘要。例如，要每 100 个训练步骤记录一次 loss 的摘要，请运行以下代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ebac2683-0720-4549-975e-7fd7fc3c49fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = './temp/tb/'\n",
    "writer = tf.summary.create_file_writer(logdir)  # Creates a summary file writer for the given log directory\n",
    "\n",
    "steps = 1000\n",
    "with writer.as_default():\n",
    "    for i in range(steps):\n",
    "        step = i + 1\n",
    "        loss = 1 - 0.001 * step\n",
    "        if step % 100 == 0:\n",
    "            tf.summary.scalar('loss', loss, step=step)  # Write a scalar summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "648adeb5-052c-4905-9ed3-47a7ee0e0df6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "events.out.tfevents.1634041761.spare-unknown113697.i.nease.net.4332.1261432.v2\n",
      "events.out.tfevents.1634041890.spare-unknown113697.i.nease.net.4332.1261440.v2\n"
     ]
    }
   ],
   "source": [
    "!ls ./temp/tb/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a46b57-e8e2-44cd-b93e-0442c04d4f62",
   "metadata": {},
   "source": [
    "#### 自动微分高级主题\n",
    "##### 动态模型\n",
    "tf.GradientTape 也可以用于动态模型。下面这个回溯线搜索算法示例看起来就像普通的 NumPy 代码，但它的控制流比较复杂，存在梯度且可微分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dfd73880-8e12-4fa0-9c60-75fbf5afb71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def line_search_step(fn, init_x, rate=1.0):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Variables are automatically tracked.\n",
    "        # But to calculate a gradient from a tensor, you must `watch` it.\n",
    "        tape.watch(init_x)\n",
    "        value = fn(init_x)\n",
    "    grad = tape.gradient(value, init_x)\n",
    "    grad_norm = tf.reduce_sum(grad*grad)\n",
    "    init_value = value\n",
    "    while value > init_value - rate * grad_norm:\n",
    "        x = init_x - rate * grad\n",
    "        value = fn(x)\n",
    "        rate /= 2.0\n",
    "    return x, value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f64c8d7-acea-4c09-80d4-244e2ce28c31",
   "metadata": {},
   "source": [
    "##### 自定义梯度\n",
    "自定义梯度是重写梯度的一种简单方法。在前向函数中，定义相对于输入、输出或中间结果的梯度。例如，下面是在后向传递中裁剪梯度范数的一种简单方法\n",
    "\n",
    "自定义梯度通常用来为运算序列提供数值稳定的梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "09e50885-a22c-4790-bb50-a29e52146855",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.custom_gradient\n",
    "def clip_gradient_by_norm(x, norm):\n",
    "    y = tf.identity(x)\n",
    "    def grad_fn(dresult):\n",
    "        return [tf.clip_by_norm(dresult, norm), None]\n",
    "    return y, grad_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8232bc47-96c7-4b3b-9c45-c11fe02ecdb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.5, shape=(), dtype=float32)\n",
      "tf.Tensor(nan, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "def log1pexp(x):\n",
    "    return tf.math.log(1 + tf.exp(x))\n",
    "\n",
    "def grad_log1pexp(x):\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(x)\n",
    "        value = log1pexp(x)\n",
    "    return tape.gradient(value, x)\n",
    "\n",
    "# The gradient computation works fine at x = 0.\n",
    "print(grad_log1pexp(tf.constant(0.)))\n",
    "\n",
    "# However, x = 100 fails because of numerical instability.\n",
    "print(grad_log1pexp(tf.constant(100.)))\n",
    "\n",
    "# 此时可以自定义梯度计算"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998292e8-ace9-48ff-bc6e-e102e243fe79",
   "metadata": {},
   "source": [
    "在此例中，log1pexp 函数可以通过自定义梯度进行分析简化。下面的实现重用了在前向传递期间计算的 tf.exp(x) 值，通过消除冗余计算使其变得更加高效"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ac26dd28-4e69-4891-b363-b967d9a6604a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.5, shape=(), dtype=float32)\n",
      "tf.Tensor(1.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "@tf.custom_gradient\n",
    "def log1pexp(x):\n",
    "    e = tf.exp(x)\n",
    "    def grad(dy):\n",
    "        return dy*(1 - 1 / (1+e))\n",
    "    return tf.math.log(1+e), grad\n",
    "\n",
    "def grad_log1pexp(x):\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(x)\n",
    "        value = log1pexp(x)\n",
    "    return tape.gradient(value, x)\n",
    "\n",
    "print(grad_log1pexp(tf.constant(0.)))\n",
    "print(grad_log1pexp(tf.constant(100.)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193ceb54-d9d4-4555-aa1d-5c5a4b92b770",
   "metadata": {},
   "source": [
    "#### 性能\n",
    "在 Eager Execution 期间，计算会自动分流到 GPU。如果想控制计算运行的位置，可将其放在 tf.device('/gpu:0') 块（或 CPU 等效块）中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "de3c3159-5d0a-4733-b98a-1f004103d139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to multiply a (1000, 1000) matrix by itself 200 times:\n",
      "cpu cost: 0.36700987815856934 secs\n",
      "[LogicalDevice(name='/device:CPU:0', device_type='CPU'), LogicalDevice(name='/device:GPU:0', device_type='GPU')]\n",
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "gpu cost: 0.06110048294067383 secs\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def measure(x, steps):\n",
    "    # TensorFlow initializes a GPU the first time it's used, exclude from timing.\n",
    "    tf.matmul(x, x)\n",
    "    start = time.time()\n",
    "    for i in range(steps):\n",
    "        x = tf.matmul(x, x)\n",
    "    # tf.matmul can return before completing the matrix multiplication\n",
    "    # (e.g., can return after enqueing the operation on a CUDA stream).\n",
    "    # The x.numpy() call below will ensure that all enqueued operations\n",
    "    # have completed (and will also copy the result to host memory,\n",
    "    # so we're including a little more than just the matmul operation\n",
    "    # time).\n",
    "    _ = x.numpy()\n",
    "    end = time.time()\n",
    "    return end - start\n",
    "\n",
    "shape = (1000, 1000)\n",
    "steps = 200\n",
    "print(\"Time to multiply a {} matrix by itself {} times:\".format(shape, steps))\n",
    "\n",
    "# run on cpu:\n",
    "x_test = tf.random.normal(shape)\n",
    "with tf.device('/cpu:0'):\n",
    "    print('cpu cost: {} secs'.format(measure(x_test, steps)))\n",
    "\n",
    "# run on gpu:\n",
    "print(tf.config.experimental.list_logical_devices())\n",
    "print(tf.config.experimental.list_physical_devices())\n",
    "with tf.device('/gpu:0'):\n",
    "    print('gpu cost: {} secs'.format(measure(x_test, steps)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "45359875-5445-49eb-9944-d215c57ef31b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tmp/ipykernel_4332/3185753953.py:5: _EagerTensorBase.gpu (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.identity instead.\n",
      "WARNING:tensorflow:From /tmp/ipykernel_4332/3185753953.py:6: _EagerTensorBase.cpu (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.identity instead.\n"
     ]
    }
   ],
   "source": [
    "# 可以将tf.Tensor对象复制到不同设备进行计算\n",
    "if tf.config.experimental.list_physical_devices('GPU'):\n",
    "    x = tf.random.normal([10, 10])\n",
    "    \n",
    "    x_gpu0 = x.gpu()\n",
    "    x_cpu = x.cpu()\n",
    "    \n",
    "    _ = tf.matmul(x_cpu, x_cpu)\n",
    "    _ = tf.matmul(x_gpu0, x_gpu0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17bb0a0b-fb5e-4da2-9d12-40cf07617b7d",
   "metadata": {},
   "source": [
    "##### 基准\n",
    "\n",
    "对于计算量繁重的模型（如在 GPU 上训练的 ResNet50），Eager Execution 性能与 tf.function 执行相当。但是对于计算量较小的模型来说，这种性能差距会越来越大，并且在为有大量小运算的模型优化热代码路径方面，其性能还有待提升\n",
    "\n",
    "#### 使用函数\n",
    "虽然 Eager Execution 增强了开发和调试的交互性，但 TensorFlow 1.x 样式的计算图执行在分布式训练、性能优化和生产部署方面具有优势。为了弥补这一差距，TensorFlow 2.0 通过 tf.function API 引入了 function。有关详细信息，请参阅 tf.function 指南"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d063bb-2078-4a28-b351-9eaacdef0414",
   "metadata": {},
   "source": [
    "### 张量\n",
    "\n",
    "#### 简介\n",
    "\n",
    "张量是具有统一类型（称为 dtype）的多维数组。您可以在 tf.dtypes.DType 中查看所有支持的 dtypes。\n",
    "\n",
    "如果您熟悉 NumPy，就会知道张量与 np.arrays 有一定的相似性。\n",
    "\n",
    "就像 Python 数值和字符串一样，所有张量都是不可变的：永远无法更新张量的内容，只能创建新的张量\n",
    "\n",
    "#### 基础知识"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e6faefed-0f40-4035-95d8-f89d1dd1a2d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor([2. 3. 4.], shape=(3,), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[1. 2.]\n",
      " [3. 4.]\n",
      " [5. 6.]], shape=(3, 2), dtype=float16)\n",
      "tf.Tensor(\n",
      "[[[ 0  1  2  3  4]\n",
      "  [ 5  6  7  8  9]]\n",
      "\n",
      " [[10 11 12 13 14]\n",
      "  [15 16 17 18 19]]\n",
      "\n",
      " [[20 21 22 23 24]\n",
      "  [25 26 27 28 29]]], shape=(3, 2, 5), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# 一个“标量”（或称“0 秩”张量）。标量包含单个值，但没有“轴”\n",
    "rank_0_tensor = tf.constant(4)\n",
    "print(rank_0_tensor)\n",
    "\n",
    "# “向量”（或称“1 秩”张量）就像一个值列表。向量有 1 个轴\n",
    "rank_1_tensor = tf.constant([2.0, 3.0, 4.0])\n",
    "print(rank_1_tensor)\n",
    "\n",
    "# “矩阵”（或称“2 秩”张量）有 2 个轴\n",
    "rank_2_tensor = tf.constant([[1,2], [3,4], [5,6]], dtype=tf.float16)\n",
    "print(rank_2_tensor)\n",
    "\n",
    "# 张量的轴可能更多，下面是一个包含 3 个轴的张量\n",
    "# There can be an arbitrary number of\n",
    "# axes (sometimes called \"dimensions\")\n",
    "rank_3_tensor = tf.constant([\n",
    "                            [[0, 1, 2, 3, 4],\n",
    "                            [5, 6, 7, 8, 9]],\n",
    "                            [[10, 11, 12, 13, 14],\n",
    "                            [15, 16, 17, 18, 19]],\n",
    "                            [[20, 21, 22, 23, 24],\n",
    "                            [25, 26, 27, 28, 29]],])\n",
    "\n",
    "print(rank_3_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b80d36c-a36e-4a39-8ba0-411086e6e5ff",
   "metadata": {},
   "source": [
    "张量与numpy数组的转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8261abc4-594c-4732-b0d5-3a0b155d34e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 2.]\n",
      " [3. 4.]\n",
      " [5. 6.]]\n",
      "[[1. 2.]\n",
      " [3. 4.]\n",
      " [5. 6.]]\n"
     ]
    }
   ],
   "source": [
    "print(np.array(rank_2_tensor))\n",
    "print(rank_2_tensor.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075d47b2-2ad4-4ce8-b0cc-14b19b0888a8",
   "metadata": {},
   "source": [
    "张量通常包含浮点型和整数数据，但是还有许多其他数据类型，包括：\n",
    "* 复杂的数值\n",
    "* 字符串\n",
    "tf.Tensor基类要求张量是“矩形”--也就是说，每一个轴上的每一个元素大小相同。但是，张量可以处理不同形状的特殊类型。\n",
    "* 不规则张量([参阅下文中的 RaggedTensor](https://www.tensorflow.org/guide/tensor#ragged_tensors))\n",
    "* 稀疏张量([参阅下文中的 SparseTensor](https://www.tensorflow.org/guide/tensor#sparse_tensors))\n",
    "\n",
    "张量的数学运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "25025ebd-af21-4fc0-8136-bb6ea6372b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[2 3]\n",
      " [4 5]], shape=(2, 2), dtype=int32) \n",
      "\n",
      "tf.Tensor(\n",
      "[[1 2]\n",
      " [3 4]], shape=(2, 2), dtype=int32) \n",
      "\n",
      "tf.Tensor(\n",
      "[[3 3]\n",
      " [7 7]], shape=(2, 2), dtype=int32) \n",
      "\n",
      "tf.Tensor(\n",
      "[[2 3]\n",
      " [4 5]], shape=(2, 2), dtype=int32) \n",
      "\n",
      "tf.Tensor(\n",
      "[[1 2]\n",
      " [3 4]], shape=(2, 2), dtype=int32) \n",
      "\n",
      "tf.Tensor(\n",
      "[[3 3]\n",
      " [7 7]], shape=(2, 2), dtype=int32) \n",
      "\n",
      "find the largest value:  tf.Tensor(10.0, shape=(), dtype=float32) \n",
      "\n",
      "find the index of largest value:  tf.Tensor([1 0], shape=(2,), dtype=int64) \n",
      "\n",
      "compute softmax:  tf.Tensor(\n",
      "[[2.6894143e-01 7.3105854e-01]\n",
      " [9.9987662e-01 1.2339458e-04]], shape=(2, 2), dtype=float32) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant([[1,2], [3,4]])\n",
    "b = tf.constant([[1,1], [1,1]])\n",
    "\n",
    "print(tf.add(a, b), '\\n')\n",
    "print(tf.multiply(a, b), '\\n')\n",
    "print(tf.matmul(a, b), '\\n')\n",
    "\n",
    "print(a + b, '\\n')\n",
    "print(a * b, '\\n')\n",
    "print(a @ b, '\\n')\n",
    "\n",
    "c = tf.constant([[4.0, 5.0], [10.0, 1.0]])\n",
    "\n",
    "print('find the largest value: ', tf.reduce_max(c), '\\n')\n",
    "print('find the index of largest value: ', tf.argmax(c), '\\n')\n",
    "print('compute softmax: ', tf.nn.softmax(c), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22b96ed-56bd-4d89-9724-5f3993dbaad9",
   "metadata": {},
   "source": [
    "#### 形状简介\n",
    "\n",
    "张量有形状，下面是几个相关术语：\n",
    "* 形状：张量每个轴的长度\n",
    "* 秩：张量轴数，标量的秩为0，向量为1，矩阵为2\n",
    "* 轴或维度：张量的一个特殊维度\n",
    "* 大小：张量的总项数，即乘积shape\n",
    "\n",
    "注：虽然您可能会看到“二维张量”之类的表述，但 2 秩张量通常并不是用来描述二维空间\n",
    "\n",
    "张量和 tf.TensorShape 对象提供了方便的属性来访问\n",
    "\n",
    "虽然通常用索引来指代轴，但是您始终要记住每个轴的含义。轴一般按照从全局到局部的顺序进行排序：首先是批次轴，随后是空间维度，最后是每个位置的特征。这样，在内存中，特征向量就会位于连续的区域"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0656dc74-c27f-47b0-89c8-e7167dbc8126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type of every element:  <dtype: 'float32'>\n",
      "number of dimensions:  4\n",
      "shape of Tensor:  (3, 2, 4, 5)\n",
      "total number of elements:  120\n"
     ]
    }
   ],
   "source": [
    "rank_4_tensor = tf.zeros([3,2,4,5])\n",
    "# print(rank_4_tensor)\n",
    "print('type of every element: ', rank_4_tensor.dtype)\n",
    "print('number of dimensions: ', rank_4_tensor.ndim)\n",
    "print('shape of Tensor: ', rank_4_tensor.shape)\n",
    "print('total number of elements: ', tf.size(rank_4_tensor).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625390a7-6ac0-4785-9e32-86dcccfb2324",
   "metadata": {},
   "source": [
    "#### 索引\n",
    "\n",
    "##### 单轴索引\n",
    "TensorFlow 遵循标准 Python 索引规则（类似于在 Python 中为列表或字符串编制索引）以及 NumPy 索引的基本规则。\n",
    "\n",
    "* 索引从 0 开始编制\n",
    "* 负索引表示按倒序编制索引\n",
    "* 冒号 : 用于切片 start:stop:step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "66e7c5fe-8656-4303-a3c1-1998c0c8002d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  2  3  5  8 13 21 34]\n",
      "first:  0\n",
      "second:  1\n",
      "last:  34\n",
      "everything:  [ 0  1  2  3  5  8 13 21 34]\n",
      "before 4:  [0 1 2 3]\n",
      "from 4 to end:  [ 5  8 13 21 34]\n",
      "from 2 to 7:  [ 2  3  5  8 13]\n",
      "every other item:  [ 0  2  5 13 34]\n",
      "reversed:  [34 21 13  8  5  3  2  1  0]\n"
     ]
    }
   ],
   "source": [
    "rank_1_tensor = tf.constant([0,1,2,3,5,8,13,21,34])\n",
    "print(rank_1_tensor.numpy())\n",
    "print('first: ', rank_1_tensor[0].numpy())\n",
    "print('second: ', rank_1_tensor[1].numpy())\n",
    "print('last: ', rank_1_tensor[-1].numpy())\n",
    "\n",
    "print('everything: ', rank_1_tensor[:].numpy())\n",
    "print('before 4: ', rank_1_tensor[:4].numpy())\n",
    "print('from 4 to end: ', rank_1_tensor[4:].numpy())\n",
    "print('from 2 to 7: ', rank_1_tensor[2:7].numpy())\n",
    "print('every other item: ', rank_1_tensor[::2].numpy())\n",
    "print('reversed: ', rank_1_tensor[::-1].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e74487-9995-420e-b88d-cd30973cb71f",
   "metadata": {},
   "source": [
    "##### 多轴索引\n",
    "\n",
    "更高秩的张量通过传递多个索引来编制索引\n",
    "\n",
    "对于高秩张量的每个单独轴，遵循单轴索引原则"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2d7d3225-f284-4a2c-a57a-fb4fd91c523b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 2.]\n",
      " [3. 4.]\n",
      " [5. 6.]]\n",
      "4.0\n",
      "second row:  [3. 4.]\n",
      "second column:  [2. 4. 6.]\n",
      "last row:  [5. 6.]\n",
      "first item in last column:  2.0\n",
      "skip the first row:  [[3. 4.]\n",
      " [5. 6.]]\n",
      "[[ 4  9]\n",
      " [14 19]\n",
      " [24 29]]\n"
     ]
    }
   ],
   "source": [
    "print(rank_2_tensor.numpy())\n",
    "print(rank_2_tensor[1, 1].numpy())\n",
    "print('second row: ', rank_2_tensor[1, :].numpy())\n",
    "print('second column: ', rank_2_tensor[:, 1].numpy())\n",
    "print('last row: ', rank_2_tensor[-1, :].numpy())\n",
    "print('first item in last column: ', rank_2_tensor[0, -1].numpy())\n",
    "print('skip the first row: ', rank_2_tensor[1:, :].numpy())\n",
    "\n",
    "# 3轴张量的索引\n",
    "print(rank_3_tensor[:, :, 4].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8a49db-9ad1-42e4-90cf-b8b3bdbc8d9d",
   "metadata": {},
   "source": [
    "#### 操作形状\n",
    "\n",
    "reshape a Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "50bf2fab-c34d-479e-bc09-8f9432f1ded7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 1)\n",
      "[3, 1]\n",
      "(1, 3)\n",
      "tf.Tensor(\n",
      "[[[ 0  1  2  3  4]\n",
      "  [ 5  6  7  8  9]]\n",
      "\n",
      " [[10 11 12 13 14]\n",
      "  [15 16 17 18 19]]\n",
      "\n",
      " [[20 21 22 23 24]\n",
      "  [25 26 27 28 29]]], shape=(3, 2, 5), dtype=int32)\n",
      "tf.Tensor(\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29], shape=(30,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# Shape returns a `TensorShape` object that shows the size on each dimension\n",
    "var_x = tf.Variable(tf.constant([[1], [2], [3]]))\n",
    "print(var_x.shape)\n",
    "print(var_x.shape.as_list())\n",
    "\n",
    "reshaped = tf.reshape(var_x, [1, 3])\n",
    "print(reshaped.shape)\n",
    "\n",
    "# 数据在内存中的布局保持不变，同时使用请求的形状创建一个指向同一数据的新张量。TensorFlow 采用 C 样式的“行优先”内存访问顺序，即最右侧的索引值递增对应于内存中的单步位移\n",
    "print(rank_3_tensor)\n",
    "\n",
    "# 如果您展平张量，则可以看到它在内存中的排列顺序\n",
    "print(tf.reshape(rank_3_tensor, [-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b7e839-13f2-42fd-9dbe-daf19372652a",
   "metadata": {},
   "source": [
    "一般来说，tf.reshape 唯一合理的用途是用于合并或拆分相邻轴（或添加/移除 1）。\n",
    "\n",
    "对于 3x2x5 张量，重构为 (3x2)x5 或 3x(2x5) 都合理，因为切片不会混淆\n",
    "\n",
    "重构可以处理总元素个数相同的任何新形状，但是如果不遵从轴的顺序，则不会发挥任何作用。\n",
    "\n",
    "利用 tf.reshape 无法实现轴的交换，要交换轴，您需要使用 tf.transpose\n",
    "\n",
    "* 一些错误的示例\n",
    "\n",
    "```python\n",
    "# Bad examples: don't do this\n",
    "\n",
    "# You can't reorder axes with reshape.\n",
    "print(tf.reshape(rank_3_tensor, [2, 3, 5]), \"\\n\") \n",
    "\n",
    "# This is a mess\n",
    "print(tf.reshape(rank_3_tensor, [5, 6]), \"\\n\")\n",
    "\n",
    "# This doesn't work at all\n",
    "try:\n",
    "  tf.reshape(rank_3_tensor, [7, -1])\n",
    "except Exception as e:\n",
    "  print(f\"{type(e).__name__}: {e}\")\n",
    "```\n",
    "\n",
    "可能会遇到非完全指定的形状。要么是形状包含 None（轴长度未知），要么是整个形状为 None（张量的秩未知）。\n",
    "\n",
    "除了 tf.RaggedTensor 外，这种情况只会在 TensorFlow 的符号化计算图构建 API 环境中出现\n",
    "\n",
    "* tf.function\n",
    "* Keras函数式API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "97a5ed6c-1c2e-4fb9-bb62-93a773005c92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 0  1  2  3  4]\n",
      " [ 5  6  7  8  9]\n",
      " [10 11 12 13 14]\n",
      " [15 16 17 18 19]\n",
      " [20 21 22 23 24]\n",
      " [25 26 27 28 29]], shape=(6, 5), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[ 0  1  2  3  4  5  6  7  8  9]\n",
      " [10 11 12 13 14 15 16 17 18 19]\n",
      " [20 21 22 23 24 25 26 27 28 29]], shape=(3, 10), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "print(tf.reshape(rank_3_tensor, [3*2, 5]))\n",
    "print(tf.reshape(rank_3_tensor, [3, -1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c4d0de-db6f-40ab-b14b-b24d7f73d33d",
   "metadata": {},
   "source": [
    "#### DTypes详解\n",
    "\n",
    "使用 Tensor.dtype 属性可以检查 tf.Tensor 的数据类型。\n",
    "\n",
    "从 Python 对象创建 tf.Tensor 时，您可以选择指定数据类型。\n",
    "\n",
    "如果不指定，TensorFlow 会选择一个可以表示您的数据的数据类型。TensorFlow 将 Python 整数转换为 tf.int32，将 Python 浮点数转换为 tf.float32。另外，当转换为数组时，TensorFlow 会采用与 NumPy 相同的规则。\n",
    "\n",
    "数据类型可以相互转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ff51cd8e-328c-40dd-9bd1-6e0064cb432d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([2 3 4], shape=(3,), dtype=uint8)\n"
     ]
    }
   ],
   "source": [
    "the_f64_tensor = tf.constant([2.2, 3.3, 4.4], dtype=tf.float64)\n",
    "the_f16_tensor = tf.cast(the_f64_tensor, dtype=tf.float16)\n",
    "# Now, let's cast to an uint8 and lose the decimal precision\n",
    "the_u8_tensor = tf.cast(the_f16_tensor, dtype=tf.uint8)\n",
    "print(the_u8_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7181a8d-8364-42dd-a8a9-d69271d8e90a",
   "metadata": {},
   "source": [
    "#### 广播\n",
    "\n",
    "广播是从 NumPy 中的等效功能借用的一个概念。简而言之，在一定条件下，对一组张量执行组合运算时，为了适应大张量，会对小张量进行“扩展”。\n",
    "\n",
    "最简单和最常见的例子是尝试将张量与标量相乘或相加。在这种情况下会对标量进行广播，使其变成与其他参数相同的形状"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c7548b49-e570-4049-9e18-32d1ca4f5b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([2 4 6], shape=(3,), dtype=int32)\n",
      "tf.Tensor([2 4 6], shape=(3,), dtype=int32)\n",
      "tf.Tensor([2 4 6], shape=(3,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant([1,2,3])\n",
    "y = tf.constant(2)\n",
    "z = tf.constant([2,2,2])\n",
    "# All of these are the same computation\n",
    "print(tf.multiply(x, 2))\n",
    "print(x*y)\n",
    "print(x*z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60085799-4554-408b-bfd6-704354882270",
   "metadata": {},
   "source": [
    "同样，可以扩展长度为 1 的轴，使其匹配其他参数。在同一个计算中可以同时扩展两个参数。\n",
    "\n",
    "在本例中，一个 3x1 的矩阵与一个 1x4 进行元素级乘法运算，从而产生一个 3x4 的矩阵。注意前导 1 是可选的：y 的形状是 [4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8d468fe5-4aa5-4d05-abf0-530de1698fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1]\n",
      " [2]\n",
      " [3]], shape=(3, 1), dtype=int32)\n",
      "tf.Tensor([1 2 3 4], shape=(4,), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[ 1  2  3  4]\n",
      " [ 2  4  6  8]\n",
      " [ 3  6  9 12]], shape=(3, 4), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[ 1  2  3  4]\n",
      " [ 2  4  6  8]\n",
      " [ 3  6  9 12]], shape=(3, 4), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# these are the same computations\n",
    "x = tf.reshape(x, [3, 1])\n",
    "y = tf.range(1, 5)\n",
    "print(x)\n",
    "print(y)\n",
    "print(tf.multiply(x, y))  # 广播相加：[3, 1] 乘以 [1, 4] 的结果是 [3,4]\n",
    "\n",
    "# 下面是不使用广播的同一运算\n",
    "\n",
    "x_stretch = tf.constant([[1, 1, 1, 1],\n",
    "                         [2, 2, 2, 2],\n",
    "                         [3, 3, 3, 3]])\n",
    "\n",
    "y_stretch = tf.constant([[1, 2, 3, 4],\n",
    "                         [1, 2, 3, 4],\n",
    "                         [1, 2, 3, 4]])\n",
    "\n",
    "print(x_stretch * y_stretch)  # Again, operator overloading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5d5e87-bef2-4746-be1c-d8c6606b5593",
   "metadata": {},
   "source": [
    "在大多数情况下，广播的时间和空间效率更高，因为广播运算不会在内存中具体化扩展的张量。\n",
    "\n",
    "使用 tf.broadcast_to 可以了解广播的运算方式\n",
    "\n",
    "与数学运算不同，broadcast_to 并不会节省内存。在这个示例中，您将具体化张量。\n",
    "\n",
    "这可能会变得更复杂。Jake VanderPlas 的《Python 数据科学手册》一书中的这一节介绍了更多广播技巧（同样使用 NumPy）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b1212ba0-e24f-4d74-b32b-1cfc2db65269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1 2 3]\n",
      " [1 2 3]\n",
      " [1 2 3]], shape=(3, 3), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "print(tf.broadcast_to(tf.constant([1,2,3]), [3,3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a03daf-3def-44f1-842a-239dca7307df",
   "metadata": {},
   "source": [
    "#### tf.convert_to_tensor\n",
    "\n",
    "大部分运算（如 tf.matmul 和 tf.reshape）会使用 tf.Tensor 类的参数。不过，在上面的示例中，您会发现形状类似于张量的 Python 对象也可以接受。\n",
    "\n",
    "大部分（但并非全部）运算会在非张量参数上调用 convert_to_tensor。我们提供了一个转换注册表，大多数对象类（如 NumPy 的 ndarray、TensorShape、Python 列表和 tf.Variable）都可以自动转换。\n",
    "\n",
    "有关更多详细信息，请参阅 tf.register_tensor_conversion_function。如果您有自己的类型，则可能希望自动转换为张量\n",
    "\n",
    "#### 不规则张量\n",
    "\n",
    "如果张量的某个轴上的元素个数可变，则称为“不规则”张量。对于不规则数据，请使用 tf.ragged.RaggedTensor。\n",
    "\n",
    "例如，下面的例子无法用规则张量表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b1ad6455-8986-492c-80eb-c2b73ef53932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ValueError: Can't convert non-rectangular Python sequence to Tensor.\n",
      "<tf.RaggedTensor [[0, 1, 2, 3], [4, 5], [6, 7, 8], [9]]>\n",
      "(4, None)\n"
     ]
    }
   ],
   "source": [
    "ragged_list = [\n",
    "    [0, 1, 2, 3],\n",
    "    [4, 5],\n",
    "    [6, 7, 8],\n",
    "    [9]\n",
    "]\n",
    "\n",
    "try:\n",
    "    tensor = tf.constant(ragged_list)\n",
    "except Exception as e:\n",
    "    print(f\"{type(e).__name__}: {e}\")\n",
    "    \n",
    "ragged_tensor = tf.ragged.constant(ragged_list)\n",
    "print(ragged_tensor)\n",
    "print(ragged_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3133dbd8-c27e-415a-a01f-a535ac62c570",
   "metadata": {},
   "source": [
    "#### 字符串张量\n",
    "\n",
    "tf.string 是一种 dtype，也就是说，在张量中，您可以用字符串（可变长度字节数组）来表示数据。\n",
    "\n",
    "字符串是原子类型，无法像 Python 字符串一样编制索引。字符串的长度并不是张量的一个轴。有关操作字符串的函数，请参阅 tf.strings。\n",
    "\n",
    "下面是一个标量字符串张量："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "04cc4ca3-8c3f-4c46-a9d3-d41c6ef6cbf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'Gray wolf', shape=(), dtype=string)\n",
      "tf.Tensor([b'Gray wolf' b'Quick brown fox' b'Lazy dog'], shape=(3,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "# Tensors can be strings, too here is a scalar string\n",
    "scalar_string_tensor = tf.constant('Gray wolf')\n",
    "print(scalar_string_tensor)\n",
    "\n",
    "# If we have three string tensors of different lengths, this is OK.\n",
    "tensor_of_strings = tf.constant([\"Gray wolf\",\n",
    "                                 \"Quick brown fox\",\n",
    "                                 \"Lazy dog\"])\n",
    "print(tensor_of_strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66e945a-7c27-4dbc-b3b0-972a415d63fc",
   "metadata": {},
   "source": [
    "在上面的打印输出中，b 前缀表示 tf.string dtype 不是 Unicode 字符串，而是字节字符串。有关在 TensorFlow 如何使用 Unicode 文本的详细信息，请参阅 Unicode 教程。\n",
    "\n",
    "如果传递 Unicode 字符，则会使用 utf-8 编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "405eebff-17b1-4574-b4b1-98c7d9b54b06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'\\xf0\\x9f\\xa5\\xb3\\xf0\\x9f\\x91\\x8d'>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.constant(\"🥳👍\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "605afd93-b957-40a0-8e10-1cf7282305e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([b'Gray' b'wolf'], shape=(2,), dtype=string)\n",
      "<tf.RaggedTensor [[b'Gray', b'wolf'], [b'Quick', b'brown', b'fox'], [b'Lazy', b'dog']]>\n"
     ]
    }
   ],
   "source": [
    "print(tf.strings.split(scalar_string_tensor, sep=' '))\n",
    "\n",
    "# ...but it turns into a `RaggedTensor` if we split up a tensor of strings,\n",
    "# as each string might be split into a different number of parts.\n",
    "print(tf.strings.split(tensor_of_strings, sep=' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72d451b-515c-4dbc-bbef-a3ebd5a63b30",
   "metadata": {},
   "source": [
    "tf.strings.to_number\n",
    "\n",
    "虽然不能使用 tf.cast 将字符串张量转换为数值，但是可以先将其转换为字节，然后转换为数值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "dce94905-c60c-4eae-9efd-b6f8c7428548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([  1.  10. 100.], shape=(3,), dtype=float32)\n",
      "tf.Tensor([b'D' b'u' b'c' b'k'], shape=(4,), dtype=string)\n",
      "tf.Tensor([ 68 117  99 107], shape=(4,), dtype=uint8)\n",
      "\n",
      "Unicode bytes: tf.Tensor(b'\\xe3\\x82\\xa2\\xe3\\x83\\x92\\xe3\\x83\\xab \\xf0\\x9f\\xa6\\x86', shape=(), dtype=string)\n",
      "\n",
      "Unicode chars: tf.Tensor([b'\\xe3\\x82\\xa2' b'\\xe3\\x83\\x92' b'\\xe3\\x83\\xab' b' ' b'\\xf0\\x9f\\xa6\\x86'], shape=(5,), dtype=string)\n",
      "\n",
      "Unicode values: tf.Tensor([ 12450  12498  12523     32 129414], shape=(5,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "text = tf.constant('1 10 100')\n",
    "print(tf.strings.to_number(tf.strings.split(text)))\n",
    "\n",
    "byte_strings = tf.strings.bytes_split(tf.constant(\"Duck\"))\n",
    "byte_ints = tf.io.decode_raw(tf.constant('Duck'), tf.uint8)\n",
    "print(byte_strings)\n",
    "print(byte_ints)\n",
    "\n",
    "# Or split it up as unicode and then decode it\n",
    "unicode_bytes = tf.constant(\"アヒル 🦆\")\n",
    "unicode_char_bytes = tf.strings.unicode_split(unicode_bytes, \"UTF-8\")\n",
    "unicode_values = tf.strings.unicode_decode(unicode_bytes, \"UTF-8\")\n",
    "\n",
    "print(\"\\nUnicode bytes:\", unicode_bytes)\n",
    "print(\"\\nUnicode chars:\", unicode_char_bytes)\n",
    "print(\"\\nUnicode values:\", unicode_values)\n",
    "\n",
    "# tf.string dtype 可用于 TensorFlow 中的所有原始字节数据。\n",
    "# tf.io 模块包含在数据与字节类型之间进行相互转换的函数，包括解码图像和解析 csv 的函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e233732-52c4-4d43-9a97-dc30052e90bb",
   "metadata": {},
   "source": [
    "#### 稀疏张量\n",
    "\n",
    "在某些情况下，数据很稀疏，比如说在一个非常宽的嵌入空间中。为了高效存储稀疏数据，TensorFlow 支持 tf.sparse.SparseTensor 和相关运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "e3700072-c63b-403f-abcb-076587413034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparseTensor(indices=tf.Tensor(\n",
      "[[0 0]\n",
      " [1 2]], shape=(2, 2), dtype=int64), values=tf.Tensor([1 2], shape=(2,), dtype=int32), dense_shape=tf.Tensor([3 4], shape=(2,), dtype=int64))\n",
      "tf.Tensor(\n",
      "[[1 0 0 0]\n",
      " [0 0 2 0]\n",
      " [0 0 0 0]], shape=(3, 4), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "sparse_tensor = tf.sparse.SparseTensor(indices=[[0, 0], [1, 2]], values=[1,2], dense_shape=[3, 4])\n",
    "print(sparse_tensor)\n",
    "\n",
    "# convert sparse tensor to dense\n",
    "print(tf.sparse.to_dense(sparse_tensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71423b39-bddb-4247-940e-d21aa864342d",
   "metadata": {},
   "source": [
    "### 变量\n",
    "\n",
    "#### 简介\n",
    "\n",
    "TensorFlow 变量是用于表示程序处理的共享持久状态的推荐方法。本指南介绍在 TensorFlow 中如何创建、更新和管理 tf.Variable 的实例。\n",
    "\n",
    "变量通过 tf.Variable 类进行创建和跟踪。tf.Variable 表示张量，对它执行运算可以改变其值。利用特定运算可以读取和修改此张量的值。更高级的库（如 tf.keras）使用 tf.Variable 来存储模型参数\n",
    "\n",
    "#### 设置\n",
    "\n",
    "本笔记本讨论变量布局。如果要查看变量位于哪一个设备上，请取消注释这一行代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "4a69cfe7-7dc7-4531-9379-68aa34bd8500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to see where your variables get placed (see below)\n",
    "tf.debugging.set_log_device_placement(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f989c711-508f-46eb-b73f-bf5fca3470c6",
   "metadata": {},
   "source": [
    "#### 创建变量\n",
    "\n",
    "要创建变量，请提供一个初始值。tf.Variable 与初始值的 dtype 相同\n",
    "\n",
    "变量与张量的定义方式和操作行为都十分相似，实际上，它们都是 tf.Tensor 支持的一种数据结构。与张量类似，变量也有 dtype 和形状，并且可以导出至 NumPy\n",
    "\n",
    "大部分张量运算在变量上也可以按预期运行，不过变量==无法重构形状==\n",
    "\n",
    "变量由张量提供支持。您可以使用 tf.Variable.assign 重新分配张量。调用 assign（通常）不会分配新张量，而会重用现有张量的内存\n",
    "\n",
    "如果在运算中像使用张量一样使用变量，那么通常会对支持张量执行运算。\n",
    "\n",
    "从现有变量创建新变量会复制支持张量。两个变量不能共享同一内存空间\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "648aaae0-f36c-429c-8bff-cc77a13608d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(2,) dtype=complex128, numpy=array([5.+4.j, 6.+1.j])>\n",
      "(2,)\n",
      "<dtype: 'complex128'>\n",
      "<bound method BaseResourceVariable.numpy of <tf.Variable 'Variable:0' shape=(2,) dtype=complex128, numpy=array([5.+4.j, 6.+1.j])>>\n",
      "<tf.Variable 'Variable:0' shape=(2, 2) dtype=float32, numpy=\n",
      "array([[1., 2.],\n",
      "       [3., 4.]], dtype=float32)>\n",
      "tf.Tensor(\n",
      "[[1. 2.]\n",
      " [3. 4.]], shape=(2, 2), dtype=float32)\n",
      "tf.Tensor([1 1], shape=(2,), dtype=int64)\n",
      "\n",
      "Copying and reshaping:  tf.Tensor([[1. 2. 3. 4.]], shape=(1, 4), dtype=float32)\n",
      "<tf.Variable 'Variable:0' shape=(2, 2) dtype=float32, numpy=\n",
      "array([[1., 2.],\n",
      "       [3., 4.]], dtype=float32)>\n",
      "ValueError: Cannot assign to variable Variable:0 due to variable shape (2,) and value shape (3,) are incompatible\n",
      "[5. 6.]\n",
      "[2. 3.]\n",
      "[7. 9.]\n",
      "[0. 0.]\n"
     ]
    }
   ],
   "source": [
    "my_tensor = tf.constant([[1.0,2.0], [3.0,4.0]])\n",
    "my_variable = tf.Variable(my_tensor)\n",
    "\n",
    "# variable can be all kinds of types ,just like tensors\n",
    "bool_variable = tf.Variable([False, False, False, True])\n",
    "complex_variable = tf.Variable([5+4j, 6+1j])\n",
    "print(complex_variable)\n",
    "print(complex_variable.shape)\n",
    "print(complex_variable.dtype)\n",
    "print(complex_variable.numpy)\n",
    "\n",
    "print(my_variable)\n",
    "print(tf.convert_to_tensor(my_variable))\n",
    "print(tf.argmax(my_variable))\n",
    "# This creates a new tensor; it does not reshape the variable.\n",
    "print(\"\\nCopying and reshaping: \", tf.reshape(my_variable, ([1,4])))\n",
    "print(my_variable)\n",
    "\n",
    "a = tf.Variable([2.0, 3.0])\n",
    "a.assign([1, 2])\n",
    "# Not allowed as it resizes the variable: \n",
    "try:\n",
    "    a.assign([1.0, 2.0, 3.0])\n",
    "except Exception as e:\n",
    "    print(f\"{type(e).__name__}: {e}\")\n",
    "\n",
    "a = tf.Variable([2.0,3.0])\n",
    "# Create b based on the value of a\n",
    "b = tf.Variable(a)\n",
    "a.assign([5,6])\n",
    "\n",
    "# a and b are different\n",
    "print(a.numpy())\n",
    "print(b.numpy())\n",
    "\n",
    "# There are other versions of assign\n",
    "print(a.assign_add([2,3]).numpy())\n",
    "print(a.assign_sub([7,9]).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2da70a1-39ec-4565-ba6a-f2d868c4ce1e",
   "metadata": {},
   "source": [
    "#### 生命周期、命名和监视\n",
    "\n",
    "在基于 Python 的 TensorFlow 中，tf.Variable 实例与其他 Python 对象的生命周期相同。如果没有对变量的引用，则会自动将其解除分配。\n",
    "\n",
    "为了便于跟踪和调试，您还可以为变量命名。两个变量可以使用相同的名称\n",
    "\n",
    "保存和加载模型时会保留变量名。默认情况下，模型中的变量会自动获得唯一变量名，所以除非您希望自行命名，否则不必多此一举。\n",
    "\n",
    "虽然变量对微分很重要，但某些变量不需要进行微分。在创建时，通过将 trainable 设置为 False 可以关闭梯度。例如，训练计步器就是一个不需要梯度的变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "f779900c-cec7-4b2f-80e0-19e59aaf97d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[False False]\n",
      " [False False]], shape=(2, 2), dtype=bool)\n",
      "<tf.Variable 'Variable:0' shape=() dtype=int32, numpy=1>\n"
     ]
    }
   ],
   "source": [
    "# Create a and b; they have the same value but are backed by different tensors.\n",
    "a = tf.Variable(my_tensor, name='Mark')\n",
    "# A new variable with the same name, but different value\n",
    "# Note that the scalar add is broadcast\n",
    "b = tf.Variable(my_tensor+1, name='Mark')\n",
    "\n",
    "# These are elementwise-unequal, despite having the same name\n",
    "print(a == b)\n",
    "\n",
    "step_counter = tf.Variable(1, trainable=False)\n",
    "print(step_counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae0eed4-f1a4-4f83-aaed-ef6460a369de",
   "metadata": {},
   "source": [
    "#### 放置变量和张量\n",
    "\n",
    "为了提高性能，TensorFlow 会尝试将张量和变量放在与其 dtype 兼容的最快设备上。这意味着如果有 GPU，那么大部分变量都会放置在 GPU 上。\n",
    "\n",
    "不过，我们可以重写变量的位置。在以下代码段中，即使存在可用的 GPU，我们也可以将一个浮点张量和一个变量放置在 CPU 上。通过打开设备分配日志记录（参阅设置），可以查看变量的所在位置。\n",
    "\n",
    "注：虽然可以手动放置变量，但使用分布策略是一种可优化计算的更便捷且可扩展的方式。\n",
    "\n",
    "如果在有 GPU 和没有 GPU 的不同后端上运行此笔记本，则会看到不同的记录。请注意，必须在会话开始时打开设备布局记录。\n",
    "\n",
    "可以将变量或张量的位置设置在一个设备上，然后在另一个设备上执行计算。但这样会产生延迟，因为需要在两个设备之间复制数据。\n",
    "\n",
    "不过，如果您有多个 GPU 工作进程，但希望变量只有一个副本，则可以这样做\n",
    "\n",
    "注：由于 tf.config.set_soft_device_placement 默认处于打开状态，所以，即使在没有 GPU 的设备上运行此代码，它也会运行，只不过乘法步骤在 CPU 上执行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "f9fac29c-109e-4a74-83ae-734656df3c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "tf.Tensor(\n",
      "[[22. 28.]\n",
      " [49. 64.]], shape=(2, 2), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[ 1.  4.  9.]\n",
      " [ 4. 10. 18.]], shape=(2, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "tf.config.set_soft_device_placement(True)\n",
    "print(tf.config.experimental.list_physical_devices())\n",
    "with tf.device('GPU:0'):\n",
    "    # create some tensors\n",
    "    a = tf.Variable([[1.0,2.0,3.0], [4.0,5.0,6.0]])\n",
    "    b = tf.constant([[1.0,2.0], [3.0,4.0], [5.0,6.0]])\n",
    "    c = tf.matmul(a, b)\n",
    "print(c)\n",
    "\n",
    "with tf.device('CPU:0'):\n",
    "    a = tf.Variable([[1.0,2.0,3.0], [4.0,5.0,6.0]])\n",
    "    b = tf.constant([[1.0,2.0,3.0]])\n",
    "with tf.device('GPU:0'):\n",
    "    k = a*b\n",
    "print(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aceb482-654d-4acd-a019-f4c8db848375",
   "metadata": {},
   "source": [
    "### 自动微分\n",
    "\n",
    "#### 自动微分和梯度\n",
    "\n",
    "自动微分对于实现机器学习算法（例如，用于训练神经网络的反向传播）非常有用。\n",
    "\n",
    "在本指南中，您将探索使用 TensorFlow 计算梯度的方法，尤其是在 Eager Execution 中\n",
    "\n",
    "#### 计算梯度\n",
    "\n",
    "要实现自动微分，TensorFlow 需要记住在前向传递过程中哪些运算以何种顺序发生。随后，在后向传递期间，TensorFlow 以相反的顺序遍历此运算列表来计算梯度\n",
    "\n",
    "#### 梯度带\n",
    "\n",
    "TensorFlow 为自动微分提供了 tf.GradientTape API；即计算某个计算相对于某些输入（通常是 tf.Variable）的梯度。TensorFlow 会将在 tf.GradientTape 上下文内执行的相关运算“记录”到“条带”上。TensorFlow 随后会该使用条带通过反向模式微分计算“记录的”计算的梯度\n",
    "\n",
    "记录一些运算后，使用 GradientTape.gradient(target, sources) 计算某个目标（通常是损失）相对于某个源（通常是模型变量）的梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "b5754d3c-4eae-4e21-b083-6743c3d8e258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.0\n",
      "[[ 4.689597  4.537785]\n",
      " [ 9.379194  9.07557 ]\n",
      " [14.068791 13.613356]]\n",
      "[4.689597 4.537785]\n",
      "{'w': <tf.Tensor: shape=(3, 2), dtype=float32, numpy=\n",
      "array([[ 4.689597,  4.537785],\n",
      "       [ 9.379194,  9.07557 ],\n",
      "       [14.068791, 13.613356]], dtype=float32)>, 'b': <tf.Tensor: shape=(2,), dtype=float32, numpy=array([4.689597, 4.537785], dtype=float32)>}\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable(3.0)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    y = x**2\n",
    "\n",
    "# dy = 2x * dx\n",
    "dy_dx = tape.gradient(y, x)\n",
    "print(dy_dx.numpy())\n",
    "\n",
    "# 上方示例使用标量，但是 tf.GradientTape 在任何张量上都可以轻松运行\n",
    "w = tf.Variable(tf.random.normal((3,2)), name='w')\n",
    "b = tf.Variable(tf.zeros(2, dtype=tf.float32), name='b')\n",
    "x = [[1.,2.,3.]]\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    y = x @ w + b\n",
    "    loss = tf.reduce_mean(y**2)\n",
    "[dl_dw, dl_db] = tape.gradient(loss, [w, b])\n",
    "print(dl_dw.numpy())\n",
    "print(dl_db.numpy())\n",
    "\n",
    "# 要获得 loss 相对于两个变量的梯度，可以将这两个变量同时作为 gradient 方法的源传递。\n",
    "# 梯度带在关于源的传递方式上非常灵活，可以接受列表或字典的任何嵌套组合，\n",
    "# 并以相同的方式返回梯度结构（请参阅 tf.nest）\n",
    "my_vars = {\n",
    "    'w': w,\n",
    "    'b': b\n",
    "}\n",
    "grad = tape.gradient(loss, my_vars)\n",
    "print(grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc830df0-0aeb-4e18-b050-a20b2e278315",
   "metadata": {},
   "source": [
    "#### 相对于模型的梯度\n",
    "\n",
    "通常将 tf.Variables 收集到 tf.Module 或其子类之一（layers.Layer、keras.Model）中，用于设置检查点和导出。\n",
    "\n",
    "在大多数情况下，需要计算相对于模型的可训练变量的梯度。 由于 tf.Module 的所有子类都在 Module.trainable_variables 属性中聚合其变量，您可以用几行代码计算这些梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "c6a448a4-7abf-4ecd-92be-64564e05e1be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dense_3/kernel:0, [[2.6922834 0.       ]\n",
      " [5.384567  0.       ]\n",
      " [8.07685   0.       ]]\n",
      "dense_3/bias:0, [2.6922834 0.       ]\n"
     ]
    }
   ],
   "source": [
    "layer = tf.keras.layers.Dense(2, activation='relu')\n",
    "x = tf.constant([[1.,2.,3.]])\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    # forward pass\n",
    "    y = layer(x)\n",
    "    loss = tf.reduce_mean(y**2)\n",
    "\n",
    "# Calculate gradients with respect to every trainable variable\n",
    "grad = tape.gradient(loss, layer.trainable_variables)\n",
    "\n",
    "for var, g in zip(layer.trainable_variables, grad):\n",
    "    print(f'{var.name}, {g}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575ecba2-231a-4023-b8f5-b78588b069e8",
   "metadata": {},
   "source": [
    "#### 控制梯度带监视的内容\n",
    "\n",
    "默认行为是在访问可训练 tf.Variable 后记录所有运算。原因如下：\n",
    "\n",
    "* 条带需要知道在前向传递中记录哪些运算，以计算后向传递中的梯度。\n",
    "* 梯度带包含对中间输出的引用，因此应避免记录不必要的操作。\n",
    "* 最常见用例涉及计算损失相对于模型的所有可训练变量的梯度。\n",
    "\n",
    "以下示例无法计算梯度，因为默认情况下 tf.Tensor 未被“监视”，并且 tf.Variable 不可训练\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "e95d0a00-ee5e-4675-aba4-01eca91da947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(6.0, shape=(), dtype=float32)\n",
      "None\n",
      "None\n",
      "None\n",
      "['x0:0']\n"
     ]
    }
   ],
   "source": [
    "# A trainable variable\n",
    "x0 = tf.Variable(3.0, name='x0')\n",
    "# Not trainable\n",
    "x1 = tf.Variable(3.0, name='x1', trainable=False)\n",
    "# Not a Variable: A variable + tensor returns a tensor.\n",
    "x2 = tf.Variable(2.0, name='x2') + 1.0\n",
    "# Not a variable\n",
    "x3 = tf.constant(3.0, name='x3')\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    y = (x0**2) + (x1**2) + (x2**2)\n",
    "\n",
    "grad = tape.gradient(y, [x0, x1, x2, x3])\n",
    "\n",
    "for g in grad:\n",
    "    print(g)\n",
    "\n",
    "# 可以使用 GradientTape.watched_variables 方法列出梯度带正在监视的变量：\n",
    "print([var.name for var in tape.watched_variables()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d250916-e7d7-4ac1-a58e-0df4c75148fb",
   "metadata": {},
   "source": [
    "tf.GradientTape 提供了钩子，让用户可以控制被监视或不被监视的内容。\n",
    "\n",
    "要记录相对于 tf.Tensor 的梯度，您需要调用 GradientTape.watch(x)："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "43380c83-154a-4559-a3e1-c3556f75ae37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.0\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant(3.0)\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(x)\n",
    "    y = x**2\n",
    "# dy=2x*dx\n",
    "dy_dx = tape.gradient(y, x)\n",
    "print(dy_dx.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db4b910-5275-43d3-80ed-03725053cdeb",
   "metadata": {},
   "source": [
    "相反，要停用监视所有 tf.Variables 的默认行为，请在创建梯度带时设置 watch_accessed_variables=False。此计算使用两个变量，但仅连接其中一个变量的梯度："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "26f808f7-7e7f-44ef-9c50-da751f320a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x0': None, 'x1': <tf.Tensor: shape=(), dtype=float32, numpy=0.9999546>}\n"
     ]
    }
   ],
   "source": [
    "x0 = tf.Variable(0.0)\n",
    "x1 = tf.Variable(10.0)\n",
    "\n",
    "with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
    "    tape.watch(x1)\n",
    "    y0 = tf.math.sin(x0)\n",
    "    y1 = tf.nn.softplus(x1)\n",
    "    y = y0 + y1\n",
    "    ys = tf.reduce_sum(y)\n",
    "grad = tape.gradient(ys, {'x0': x0, 'x1': x1})\n",
    "\n",
    "print(grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6357c7e-5fb9-467d-ac61-5150a83a300f",
   "metadata": {},
   "source": [
    "#### 中间结果\n",
    "\n",
    "可以请求输出相对于 tf.GradientTape 上下文中计算的中间值的梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "21b1508a-7e68-4f96-8346-2ba41ba6c248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.0\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant(3.0)\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(x)\n",
    "    y = x * x\n",
    "    z = y * y\n",
    "# Use the tape to compute the gradient of z with respect to the\n",
    "# intermediate value y.\n",
    "# dz_dy = 2 * y and y = x ** 2 = 9\n",
    "print(tape.gradient(z, y).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06af5d37-e726-4dd8-b2ba-42d0e4c81719",
   "metadata": {},
   "source": [
    "默认情况下，只要调用 GradientTape.gradient 方法，就会释放 GradientTape 保存的资源。要在同一计算中计算多个梯度，请创建一个`persistent=True`的梯度带。这样一来，当梯度带对象作为垃圾回收时，随着资源的释放，可以对 gradient 方法进行多次调用。例如"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "1741dac2-b75a-4ee3-9fba-9ae33e9f63e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  4. 108.]\n",
      "[2. 6.]\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant([1, 3.0])\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    tape.watch(x)\n",
    "    y = x * x\n",
    "    z = y * y\n",
    "print(tape.gradient(z, x).numpy())\n",
    "print(tape.gradient(y, x).numpy())\n",
    "\n",
    "del tape  # drop the reference to the tape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196d5322-3d02-400a-93e4-bf6583a0b9ce",
   "metadata": {},
   "source": [
    "#### 性能说明\n",
    "\n",
    "* 在梯度带上下文内进行运算会有一个微小的开销。对于大多数 Eager Execution 来说，这一成本并不明显，但是您仍然应当仅在需要的地方使用梯度带上下文。\n",
    "\n",
    "* 梯度带使用内存来存储中间结果，包括输入和输出，以便在后向传递中使用。\n",
    "\n",
    "为了提高效率，某些运算（例如 ReLU）不需要保留中间结果，而是在前向传递中进行剪枝。不过，如果在梯度带上使用 persistent=True，则不会丢弃任何内容，并且峰值内存使用量会更高\n",
    "\n",
    "#### 非标量目标的梯度\n",
    "\n",
    "梯度从根本上说是对标量的运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "8dbf9b81-3296-419b-ba1b-0f8a6d4da38c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0\n",
      "-0.25\n",
      "3.75\n",
      "7.0\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable(2.0)\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    y0 = x**2\n",
    "    y1 = 1 / x\n",
    "print(tape.gradient(y0, x).numpy())\n",
    "print(tape.gradient(y1, x).numpy())\n",
    "\n",
    "# 因此，如果需要多个目标的梯度，则每个源的结果为：\n",
    "\n",
    "# 目标总和的梯度，或等效\n",
    "# 每个目标的梯度总和。\n",
    "x = tf.Variable(2.0)\n",
    "with tf.GradientTape() as tape:\n",
    "    y0 = x**2\n",
    "    y1 = 1 / x\n",
    "\n",
    "print(tape.gradient({'y0': y0, 'y1': y1}, x).numpy())\n",
    "\n",
    "# 类似地，如果目标不是标量，则计算总和的梯度\n",
    "x = tf.Variable(2.)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    y = x * [3., 4.]\n",
    "\n",
    "print(tape.gradient(y, x).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8f2b42-a6fc-4bb2-9ad8-9ea2bd70ed3f",
   "metadata": {},
   "source": [
    "这样一来，就可以轻松获取损失集合总和的梯度，或者逐元素损失计算总和的梯度。\n",
    "\n",
    "如果每个条目都需要单独的梯度，请参阅雅可比矩阵。\n",
    "\n",
    "在某些情况下，您可以跳过雅可比矩阵。对于逐元素计算，总和的梯度给出了每个元素相对于其输入元素的导数，因为每个元素都是独立的："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "9884d338-fc48-487d-afac-e1d0c4e58163",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = tf.linspace(-10.0, 10, 200+1)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(x)\n",
    "    y = tf.nn.sigmoid(x)\n",
    "dy_dx = tape.gradient(y, x)\n",
    "\n",
    "plt.plot(x, y, label='y')\n",
    "plt.plot(x, dy_dx, label='dy_dx')\n",
    "plt.legend()\n",
    "_ = plt.xlabel('x')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66fe593d-7006-4841-a6ab-14a346ab6172",
   "metadata": {},
   "source": [
    "#### 控制流\n",
    "\n",
    "在执行运算时，由于梯度带会记录这些运算，因此会自然地处理 Python 控制流（例如 if 和 while 语句）。\n",
    "\n",
    "此处，if 的每个分支上使用不同变量。梯度仅连接到使用的变量：\n",
    "\n",
    "注意，控制语句本身不可微分，因此对基于梯度的优化器不可见。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "58bafdbd-52c0-48a9-b522-3f68174cbcba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(1.0, shape=(), dtype=float32)\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant(1.0)\n",
    "v0 = tf.Variable(2.0)\n",
    "v1 = tf.Variable(2.0)\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    tape.watch(x)\n",
    "    if x > 0.0:\n",
    "        result = v0\n",
    "    else:\n",
    "        result = v1**2\n",
    "dv0, dv1 = tape.gradient(result, [v0, v1])\n",
    "\n",
    "print(dv0)\n",
    "print(dv1)  # 此处由于x=1.0>0.0，所以dv1的梯度没被连接，所以是None\n",
    "\n",
    "# 根据上面示例中 x 的值，梯度带将记录 result = v0 或 result = v1**2。 相对于 x 的梯度始终为 None\n",
    "dx = tape.gradient(result, x)\n",
    "print(dx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b4aa32-eb80-4bdc-8ae4-510ccfb5ee60",
   "metadata": {},
   "source": [
    "#### 获取None的梯度\n",
    "\n",
    "当目标未连接到源时，您将获得 None 的梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "e68ad264-c19c-43a0-8834-dfe56c48a1c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable(2.)\n",
    "y = tf.Variable(3.)\n",
    "with tf.GradientTape() as tape:\n",
    "    z = y * y\n",
    "print(tape.gradient(z, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b656049c-e71d-48e1-a51a-187ed7ae20da",
   "metadata": {},
   "source": [
    "此处 z 显然未连接到 x，但可以通过几种不太明显的方式将梯度断开\n",
    "\n",
    "1. 使用张量替换变量\n",
    "\n",
    "在控制梯度带监视内容部分中，梯度带会自动监视 tf.Variable，但不会监视 tf.Tensor。\n",
    "\n",
    "一个常见错误是无意中将 tf.Variable 替换为 tf.Tensor，而不使用 Variable.assign 更新 tf.Variable。见下例\n",
    "\n",
    "2. 在 TensorFlow 之外进行了计算\n",
    "如果计算退出 TensorFlow，梯度带将无法记录梯度路径。例如\n",
    "\n",
    "3. 通过整数或字符串获取梯度\n",
    "整数和字符串不可微分。如果计算路径使用这些数据类型，则不会出现梯度。\n",
    "\n",
    "谁也不会期望字符串是可微分的，但是如果不指定 dtype，很容易意外创建一个 int 常量或变量\n",
    "\n",
    "4. 通过有状态对象获取梯度\n",
    "状态会停止梯度。从有状态对象读取时，梯度带只能观察当前状态，而不能观察导致该状态的历史记录。\n",
    "\n",
    "tf.Tensor 不可变。张量创建后就不能更改。它有一个值，但没有状态。目前讨论的所有运算也都无状态：tf.matmul 的输出只取决于它的输入。\n",
    "\n",
    "tf.Variable 具有内部状态，即它的值。使用变量时，会读取状态。计算相对于变量的梯度是正常操作，但是变量的状态会阻止梯度计算进一步向后移动。 例如\n",
    "\n",
    "类似地，tf.data.Dataset 迭代器和 tf.queue 也有状态，会停止经过它们的张量上的所有梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "22ccae51-2c47-4f66-a47c-28c27990a2cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResourceVariable : tf.Tensor(1.0, shape=(), dtype=float32)\n",
      "EagerTensor : None\n",
      "None\n",
      "WARNING:tensorflow:The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.int32\n",
      "WARNING:tensorflow:The dtype of the target tensor must be floating (e.g. tf.float32) when calling GradientTape.gradient, got tf.int32\n",
      "WARNING:tensorflow:The dtype of the source tensor must be floating (e.g. tf.float32) when calling GradientTape.gradient, got tf.int32\n",
      "None\n",
      "None\n",
      "tf.Tensor(8.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 1\n",
    "x = tf.Variable(2.0)\n",
    "\n",
    "for epoch in range(2):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y = x + 1\n",
    "    print(type(x).__name__, ':', tape.gradient(y, x))\n",
    "    x = x + 1  # this should be 'x.assign_add(1)',不然会将其转为tensor，而计算不了梯度\n",
    "\n",
    "# 2\n",
    "x = tf.Variable([[1., 2.],\n",
    "                 [3., 4.]], dtype=tf.float32)\n",
    "with tf.GradientTape() as tape:\n",
    "    x2 = x**2\n",
    "    \n",
    "    # this step is calculated with numpy,即退出了tensorflow计算\n",
    "    y = np.mean(x2, axis=0)\n",
    "    \n",
    "    # Like most ops, reduce_mean will cast the NumPy array to a constant tensor\n",
    "    # using `tf.convert_to_tensor`.\n",
    "    y = tf.reduce_mean(y, axis=0)\n",
    "print(tape.gradient(y, x))\n",
    "\n",
    "# 3\n",
    "x = tf.constant(10)\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(x)\n",
    "    y = x * x\n",
    "print(tape.gradient(y, x))\n",
    "\n",
    "# 4\n",
    "x0 = tf.Variable(3.0)\n",
    "x1 = tf.Variable(1.0)\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    # update x1 = x1 + x0\n",
    "    x1.assign_add(x0)\n",
    "    # the tape starts recording from x1\n",
    "    y = x1 ** 2  # y = (x1+x0)**2\n",
    "# this doesn't work\n",
    "print(tape.gradient(y, x0))  # dy/dx0=2*(x1+x0)\n",
    "print(tape.gradient(y, x1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04fbd772-eb3f-41ef-8caf-29dd9a343210",
   "metadata": {},
   "source": [
    "#### 未注册梯度\n",
    "\n",
    "某些 tf.Operation 被注册为不可微分，将返回 None。还有一些则未注册梯度。\n",
    "\n",
    "tf.raw_ops 页面显示了哪些低级运算已经注册梯度。\n",
    "\n",
    "如果您试图通过一个没有注册梯度的浮点运算获取梯度，梯度带将抛出错误，而不是直接返回 None。这样一来，您可以了解某个环节出现问题。\n",
    "\n",
    "例如，tf.image.adjust_contrast 函数封装了 raw_ops.AdjustContrastv2，此运算可能具有梯度，但未实现该梯度：\n",
    "\n",
    "如果需要通过此运算进行微分，则需要实现梯度并注册该梯度（使用 tf.RegisterGradient），或者使用其他运算重新实现该函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "8edc4da0-3d01-487e-b67b-5278bca43a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LookupError: gradient registry has no entry for: AdjustContrastv2\n"
     ]
    }
   ],
   "source": [
    "image = tf.Variable([[[0.5,0.0,0.0]]])\n",
    "delta = tf.Variable(0.1)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    new_image = tf.image.adjust_contrast(image, delta)  # Adjust contrast of RGB or grayscale images.\n",
    "\n",
    "try:\n",
    "    print(tape.gradient(new_image, [image, delta]))\n",
    "    assert False  # this should not happen\n",
    "except Exception as e:\n",
    "    print(f'{type(e).__name__}: {e}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b6bdef-a5ac-4fda-9d88-5774e056d16b",
   "metadata": {},
   "source": [
    "#### 零而不是None\n",
    "\n",
    "在某些情况下，对于未连接的梯度，得到 0 而不是 None 会比较方便。您可以使用 unconnected_gradients 参数来决定具有未连接的梯度时返回的内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "7e8ceaf3-3234-46fc-a57a-dfec34552875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "tf.Tensor([0. 0.], shape=(2,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable([2., 2.])\n",
    "y = tf.Variable(3.)\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    z = y ** 2\n",
    "print(tape.gradient(z, x))\n",
    "print(tape.gradient(z, x, unconnected_gradients=tf.UnconnectedGradients.ZERO))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f509d8-475d-4084-9190-5b7d6c420065",
   "metadata": {},
   "source": [
    "### 图和函数简介\n",
    "\n",
    "#### 概述\n",
    "\n",
    "This is a big-picture overview that covers how `tf.function` allows you to switch from eager execution to graph execution. For a more complete specification of `tf.function`, go to the `tf.function` guide\n",
    "\n",
    "#### 什么是 图\n",
    "\n",
    "Graphs are data structures that contain a set of `tf.Operation` objects, which represent units of computation; and `tf.Tensor` objects, which represent the units of data that flow between operations\n",
    "\n",
    "![two layer network](./深度学习之吴恩达课程作业6/two-layer-network.png)\n",
    "\n",
    "* 图的好处\n",
    "\n",
    "graphs are extremely useful and let your TensorFlow run fast, run in parallel, and run efficiently on multiple devices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6375a32b-71e4-4092-a7c2-ca88af194493",
   "metadata": {},
   "source": [
    "#### steup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "dc3630ff-a8c9-4ab5-9a42-05a01e3c9a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import timeit\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d2d6d1-4c82-448f-8b09-17fbdcf6dab7",
   "metadata": {},
   "source": [
    "#### taking advantage of graphs\n",
    "\n",
    "You create and run a graph in TensorFlow by using tf.function, either as a direct call or as a decorator. tf.function takes a regular function as input and returns a Function. A Function is a Python callable that builds TensorFlow graphs from the Python function. You use a Function in the same way as its Python equivalent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "6d30049e-a445-4650-8e4c-096d8ee47744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method _EagerTensorBase.numpy of <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[12.]], dtype=float32)>>\n",
      "[[12.]]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4332/4065048709.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mtf_function_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma_function_that_uses_a_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf_function_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_value\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtf_function_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# define a python function\n",
    "def a_regular_function(x, y, b):\n",
    "    x = tf.matmul(x, y)\n",
    "    x = x + b\n",
    "    return x\n",
    "\n",
    "# tensorflow function\n",
    "a_function_that_uses_a_graph = tf.function(a_regular_function)\n",
    "\n",
    "# make some tensors.\n",
    "x1 = tf.constant([[1.0, 2.0]])\n",
    "y1 = tf.constant([[2.0], [3.0]])\n",
    "b1 = tf.constant(4.0)\n",
    "\n",
    "orig_value = a_regular_function(x1, y1, b1).numpy\n",
    "print(orig_value)\n",
    "# call a function like a python function\n",
    "tf_function_value = a_function_that_uses_a_graph(x1, y1, b1).numpy()\n",
    "print(tf_function_value)\n",
    "assert(orig_value == tf_function_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1ae1da-b189-42d7-acbd-201422e17765",
   "metadata": {},
   "source": [
    "On the outside, a Function looks like a regular function you write using TensorFlow operations. Underneath, however, it is very different. A Function encapsulates several tf.Graphs behind one API. That is how Function is able to give you the benefits of graph execution, like speed and deployability.\n",
    "\n",
    "tf.function applies to a function and all other functions it calls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846c0b82-73dc-48f8-b402-fc0e4ea8100d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
